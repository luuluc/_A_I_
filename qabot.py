import os
from langchain_community.llms import CTransformers
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.embeddings import GPT4AllEmbeddings
from langchain_community.vectorstores import FAISS

# === ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi ===

BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
model_file = os.path.join(BASE_DIR, "models", "vinallama-7b-chat_q5_0.gguf")
embedding_file = os.path.join(BASE_DIR, "models", "all-MiniLM-L6-v2-f16.gguf")
vector_db_path = os.path.join(BASE_DIR, "vectorstores", "db_faiss")


# === Load m√¥ h√¨nh LLM ===
def load_llm(model_file):
    llm = CTransformers(
        model=model_file,
        model_type="llama",
        max_new_tokens=1024,
        temperature=0.01
    )
    return llm

# === T·∫°o PromptTemplate ===
def creat_prompt(template):
    return PromptTemplate(template=template, input_variables=["context", "question"])

# === T·∫°o QA Chain ===
def create_qa_chain(prompt, llm, db):
    llm_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(search_kwargs={"k": 3}, max_tokens_limit=1024),
        return_source_documents=False,
        chain_type_kwargs={"prompt": prompt}
    )
    return llm_chain

# === ƒê·ªçc FAISS vector DB ===
def read_vectors_db():
    embedding_model = GPT4AllEmbeddings(model_file=embedding_file)
    db = FAISS.load_local(
        vector_db_path,
        embedding_model,
        allow_dangerous_deserialization=True
    )
    return db

# === Kh·ªüi ch·∫°y ===
if __name__ == "__main__":
    # B∆∞·ªõc 1: Load DB & LLM
    db = read_vectors_db()
    llm = load_llm(model_file)

    # B∆∞·ªõc 2: T·∫°o Prompt
    template = """<|im_start|>system
S·ª≠ d·ª•ng th√¥ng tin sau ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi, h√£y n√≥i kh√¥ng bi·∫øt, ƒë·ª´ng c·ªë t·∫°o ra c√¢u tr·∫£ l·ªùi.

{context}<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant"""
    prompt = creat_prompt(template)

    # B∆∞·ªõc 3: T·∫°o chain v√† ƒë·∫∑t c√¢u h·ªèi
    llm_chain = create_qa_chain(prompt, llm, db)

    question = "LoRa c√≥ th·ªÉ duy tr√¨ k·∫øt n·ªëi v√† chia s·∫ª d·ªØ li·ªáu trong th·ªùi gian l√™n ƒë·∫øn bao nhi√™u nƒÉm ?"
    response = llm_chain.invoke({"query": question})
    print("\nüìå K·∫øt qu·∫£ tr·∫£ l·ªùi:")
    print(response)
